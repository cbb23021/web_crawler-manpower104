{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "import pprint as pp\n",
    "\n",
    "# Create a directory to saving files\n",
    "file_path = r'./manpower104'\n",
    "if not os.path.exists(file_path): os.mkdir(file_path)  \n",
    "    \n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'} \n",
    "keyword = '後端工程師'\n",
    "\n",
    "# synonym dictionary\n",
    "skill_dict = {}\n",
    "skill_path = r'./manpower104/skill/skilldict.txt'\n",
    "with open(skill_path,'r',encoding='utf-8-sig') as s:\n",
    "    s_str = s.read().split('\\n')\n",
    "for each_row in s_str:\n",
    "    skill_dict[each_row.split(',')[0]] = 0\n",
    "# print(skill_dict)\n",
    "\n",
    "# with open(synonym_path, 'r', encoding='utf-8') as syn:\n",
    "#     syn_str = syn.read().split('\\n')\n",
    "# for each_row in syn_str:\n",
    "#     synonym_dict[each_row.split(',')[0]] = [item for item in each_row.split(',')]\n",
    "\n",
    "\n",
    "url_start = \"https://www.104.com.tw/jobs/search/?ro=0&isnew=30&keyword=%s\"%(keyword)\n",
    "res_start = requests.get(url_start, headers=headers)\n",
    "\n",
    "# get total pages of 104\n",
    "pages=int(res_start.text.split('\"totalPage\":')[1].split(',\"totalCount\"')[0])\n",
    "\n",
    "# how many pages need scrap.\n",
    "for page in range(1):\n",
    "    res_page=requests.get(url_start+\"&page=%s\"%(page))\n",
    "    soup_page = BeautifulSoup(res_page.text, 'html.parser')\n",
    "    article_page_list = soup_page.select('h2.b-tit a')\n",
    "\n",
    "# create title && title url\n",
    "    for article in article_page_list:\n",
    "        article_title = article.text\n",
    "        url_article = article['href'].replace('//', 'https://')\n",
    "        url_article_json = 'https://www.104.com.tw/job/ajax/content/' + url_article.split('/')[-1]\n",
    "        \n",
    "        # set new header of referer included.\n",
    "        headers_j = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36',\n",
    "                  'referer':url_article.split('?')[0]\n",
    "                    } \n",
    "        res_article = requests.get(url_article_json, headers=headers_j) \n",
    "        job_content = json.loads(res_article.text)\n",
    "\n",
    "#         pp.pprint(job_content)\n",
    "#         print(url_article)\n",
    "\n",
    "# def information\n",
    "        job_name = (job_content['data']['header']['jobName'])\n",
    "        job_company = (job_content['data']['header']['custName'])\n",
    "        job_location = (job_content['data']['jobDetail']['addressRegion'])\n",
    "        job_salary = (job_content['data']['jobDetail']['salary'])\n",
    "        job_years = (job_content['data']['condition']['workExp'])\n",
    "        job_skill = (job_content['data']['jobDetail']['jobDescription']+job_content['data']['condition']['other'])\n",
    "#         print(job_skill)\n",
    "        \n",
    "# sum information       \n",
    "        job_allinfo = ''\n",
    "        job_allinfo += '工作名稱: %s\\n' %(job_name)\n",
    "        job_allinfo += '公司名稱: %s\\n'%(job_company)\n",
    "        job_allinfo += 'URL: %s\\n'%(url_article)\n",
    "        job_allinfo += '公司地點: %s\\n'%(job_location)\n",
    "        job_allinfo += '工作薪水: %s\\n'%(job_salary)\n",
    "        job_allinfo += '工作經歷: %s\\n'%(job_years)\n",
    "        \n",
    "#         print(type(job_skill))\n",
    "#         print(job_skill)\n",
    "#         print(job_allinfo)\n",
    "#         print('='*50)\n",
    "\n",
    "# count skill       \n",
    "        wordlist = jieba.cut(str(job_skill).lower())\n",
    "        jieba.load_userdict('./manpower104/skill/skilldict.txt')\n",
    "        cutlist = [i for i in wordlist]\n",
    "        for w in skill_dict:\n",
    "            if w in cutlist:\n",
    "                job_allinfo += '%s: 1\\n'%(w)\n",
    "            else:\n",
    "                job_allinfo += '%s: 0\\n'%(w)  \n",
    "        print(job_allinfo)\n",
    "        print('='*50)\n",
    "    \n",
    "# save file\n",
    "        try:\n",
    "            with open( '%s/%s.txt' %(file_path, job_company+'_'+article_title), 'w', encoding='utf-8-sig') as f:\n",
    "                f.write(job_allinfo)\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            article_title = article_title.replace('/','&&')\n",
    "            \n",
    "            print(len(article_page_list))\n",
    "            print(article_title)\n",
    "            print(url_article)\n",
    "            print(e.args)\n",
    "            with open( '%s/%s.txt' %(file_path, job_company+'_'+article_title), 'w', encoding='utf-8-sig') as f:\n",
    "                f.write(job_allinfo)\n",
    "            \n",
    "        except OSError as e:\n",
    "            print('==========')\n",
    "            print(url_article)\n",
    "            print(e.args)\n",
    "            print('==========')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create form table\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "filelist = glob.glob('./manpower104/*.txt')\n",
    "\n",
    "rowData = []\n",
    "\n",
    "for filename in filelist:\n",
    "    with open(filename, 'r', encoding='utf-8-sig') as f:\n",
    "        tmplist = f.read().split('\\n')[0:-1]\n",
    "        rowData.append(tmplist)       \n",
    "        print(rowData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create column\n",
    "columns = [r.split(': ')[0] for r in rowData[0]]\n",
    "\n",
    "# create table\n",
    "df = pd.DataFrame(data=rowData, columns=columns)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove title words\n",
    "for c in df:\n",
    "    df[c] = df[c].apply(lambda x: x.split(': ')[-1])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cvs.file\n",
    "df.to_csv('./manpower104/manpower_table.csv') \n",
    "print('done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
